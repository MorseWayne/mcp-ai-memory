# =============================================================================
# Mem0 Local MCP Server 配置
# =============================================================================

# -----------------------------------------------------------------------------
# 传输配置
# -----------------------------------------------------------------------------
# 传输协议: 'sse' (HTTP Server-Sent Events) 或 'stdio' (标准输入输出)
TRANSPORT=sse

# SSE 传输时的主机地址 (stdio 模式下可留空)
HOST=0.0.0.0

# SSE 传输时的端口 (stdio 模式下可留空)
PORT=8050

# -----------------------------------------------------------------------------
# LLM 配置 (用于记忆处理和提取)
# -----------------------------------------------------------------------------
# LLM 提供商: openai, openrouter, ollama, azure_openai, deepseek, together, groq
LLM_PROVIDER=openai

# LLM API 基础 URL
# OpenAI: https://api.openai.com/v1
# Ollama: http://localhost:11434
# OpenRouter: https://openrouter.ai/api/v1
# Azure OpenAI: https://<your-resource>.openai.azure.com
# DeepSeek: https://api.deepseek.com/v1
# Together: https://api.together.xyz/v1
# Groq: https://api.groq.com/openai/v1
LLM_BASE_URL=https://api.openai.com/v1

# LLM API 密钥 (Ollama 本地部署通常不需要)
LLM_API_KEY=sk-xxx

# LLM 模型选择
# OpenAI: gpt-4o-mini, gpt-4o, gpt-4-turbo
# Ollama: qwen2.5:7b, llama3.1:8b, mistral:7b
# OpenRouter: anthropic/claude-3.5-sonnet, openai/gpt-4o
# DeepSeek: deepseek-chat, deepseek-reasoner
# Together: meta-llama/Llama-3-70b-chat-hf
# Groq: llama-3.1-70b-versatile, mixtral-8x7b-32768
LLM_MODEL=gpt-4o-mini

# LLM 温度参数 (0.0-1.0, 越低越确定性)
LLM_TEMPERATURE=0.2

# LLM 最大 token 数
LLM_MAX_TOKENS=2000

# -----------------------------------------------------------------------------
# Embedding 模型配置 (用于语义搜索)
# -----------------------------------------------------------------------------
# Embedding 提供商: openai, ollama, huggingface, azure_openai
EMBEDDING_PROVIDER=openai

# Embedding 模型
# OpenAI: text-embedding-3-small (1536维), text-embedding-3-large (3072维)
# Ollama: nomic-embed-text (768维), mxbai-embed-large (1024维)
# HuggingFace: sentence-transformers/all-MiniLM-L6-v2 (384维)
EMBEDDING_MODEL=text-embedding-3-small

# Embedding 维度 (需与模型匹配)
EMBEDDING_DIMS=1536

# Embedding API 基础 URL (如果与 LLM 不同)
# 留空则使用 LLM_BASE_URL
EMBEDDING_BASE_URL=

# Embedding API 密钥 (如果与 LLM 不同)
# 留空则使用 LLM_API_KEY
EMBEDDING_API_KEY=

# -----------------------------------------------------------------------------
# 向量数据库配置
# -----------------------------------------------------------------------------
# 向量存储提供商: qdrant, chroma, pgvector
VECTOR_STORE_PROVIDER=qdrant

# Qdrant 配置
# 本地文件存储 (无需 Qdrant 服务)
QDRANT_PATH=./mem0_data

# 是否启用 Qdrant 本地持久化 (跨重启保留数据)
# - false: 启动时可能会清空 QDRANT_PATH（适合临时/测试）
# - true: 作为本地持久化目录使用（推荐用于本地长期记忆）
QDRANT_ON_DISK=true

# Qdrant 服务器配置 (如果使用远程 Qdrant)
# QDRANT_HOST=localhost
# QDRANT_PORT=6333
# QDRANT_API_KEY=

# Qdrant 集合名称
QDRANT_COLLECTION=mem0_memories

# PostgreSQL 配置 (如果使用 pgvector)
# DATABASE_URL=postgresql://user:password@localhost:5432/mem0

# Chroma 配置 (如果使用 Chroma)
# CHROMA_PATH=./chroma_data

# -----------------------------------------------------------------------------
# 用户配置
# -----------------------------------------------------------------------------
# 默认用户 ID (当请求未指定时使用)
DEFAULT_USER_ID=default_user

# -----------------------------------------------------------------------------
# 高级配置
# -----------------------------------------------------------------------------
# 是否启用图记忆 (实验性功能)
ENABLE_GRAPH_MEMORY=false

# 日志级别: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO
