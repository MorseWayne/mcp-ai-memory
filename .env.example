# ==============================================================================
# MCP AI Memory Server 配置
# ==============================================================================
#
# 快速开始：
#   1. 复制此文件为 .env
#   2. 填写下方 [必填] 标记的配置项
#   3. 运行 `uv run mcp-ai-memory` 或 `docker compose up -d`
#
# ==============================================================================


# ------------------------------------------------------------------------------
# [必填] LLM 配置
# ------------------------------------------------------------------------------

# API 密钥
LLM_API_KEY=sk-xxx

# API 基础地址
# ┌─────────────┬─────────────────────────────────────────┐
# │ 提供商      │ BASE_URL                                │
# ├─────────────┼─────────────────────────────────────────┤
# │ OpenAI      │ https://api.openai.com/v1               │
# │ DeepSeek    │ https://api.deepseek.com/v1             │
# │ OpenRouter  │ https://openrouter.ai/api/v1            │
# │ Together    │ https://api.together.xyz/v1             │
# │ Groq        │ https://api.groq.com/openai/v1          │
# │ Ollama      │ http://localhost:11434                  │
# │ Azure       │ https://<resource>.openai.azure.com     │
# └─────────────┴─────────────────────────────────────────┘
LLM_BASE_URL=https://api.openai.com/v1

# 模型名称
# ┌─────────────┬─────────────────────────────────────────┐
# │ 提供商      │ 推荐模型                                │
# ├─────────────┼─────────────────────────────────────────┤
# │ OpenAI      │ gpt-4o-mini, gpt-4o                     │
# │ DeepSeek    │ deepseek-chat, deepseek-reasoner        │
# │ Ollama      │ qwen2.5:7b, llama3.1:8b                 │
# │ OpenRouter  │ anthropic/claude-3.5-sonnet             │
# └─────────────┴─────────────────────────────────────────┘
LLM_MODEL=gpt-4o-mini


# ------------------------------------------------------------------------------
# [可选] LLM 高级参数
# ------------------------------------------------------------------------------

LLM_PROVIDER=openai          # 提供商: openai, ollama, azure_openai, deepseek, together, groq, openrouter
LLM_TEMPERATURE=0.2          # 温度 (0.0-1.0, 越低越确定)
LLM_MAX_TOKENS=2000          # 最大输出 token 数


# ------------------------------------------------------------------------------
# [可选] Embedding 配置 (默认复用 LLM 配置)
# ------------------------------------------------------------------------------

# ┌──────────────┬─────────────────────────────┬──────────┐
# │ 提供商       │ 模型                        │ 维度     │
# ├──────────────┼─────────────────────────────┼──────────┤
# │ OpenAI       │ text-embedding-3-small      │ 1536     │
# │ OpenAI       │ text-embedding-3-large      │ 3072     │
# │ Ollama       │ nomic-embed-text            │ 768      │
# │ Ollama       │ mxbai-embed-large           │ 1024     │
# │ HuggingFace  │ all-MiniLM-L6-v2            │ 384      │
# └──────────────┴─────────────────────────────┴──────────┘

EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMS=1536

# 如果 Embedding 服务与 LLM 不同，取消注释并填写：
# EMBEDDING_BASE_URL=
# EMBEDDING_API_KEY=


# ------------------------------------------------------------------------------
# [可选] 向量数据库配置
# ------------------------------------------------------------------------------

VECTOR_STORE_PROVIDER=qdrant   # 提供商: qdrant, chroma, pgvector

# --- Qdrant (默认) ---
QDRANT_PATH=./mem0_data        # 本地存储路径 (无需 Qdrant 服务)
QDRANT_ON_DISK=true            # 持久化到磁盘
QDRANT_COLLECTION=mem0_memories

# 如果使用远程 Qdrant 服务，取消注释：
# QDRANT_HOST=localhost
# QDRANT_PORT=6333
# QDRANT_API_KEY=

# --- PostgreSQL (pgvector) ---
# DATABASE_URL=postgresql://user:password@localhost:5432/mem0

# --- Chroma ---
# CHROMA_PATH=./chroma_data


# ------------------------------------------------------------------------------
# [可选] 服务配置
# ------------------------------------------------------------------------------

TRANSPORT=sse                  # 传输协议: sse (HTTP) 或 stdio
HOST=0.0.0.0                   # 监听地址 (sse 模式)
PORT=8050                      # 监听端口 (sse 模式)
DEFAULT_USER_ID=default_user   # 默认用户 ID
LOG_LEVEL=INFO                 # 日志级别: DEBUG, INFO, WARNING, ERROR


# ------------------------------------------------------------------------------
# [可选] Docker 容器代理 (仅 docker-compose 部署时需要)
# ------------------------------------------------------------------------------
# 如果你的网络需要代理才能访问外网 API，取消注释并配置：
# 注意: 容器内应使用 host.docker.internal 访问宿主机代理

# CONTAINER_HTTP_PROXY=http://host.docker.internal:7890
# CONTAINER_HTTPS_PROXY=http://host.docker.internal:7890
# CONTAINER_NO_PROXY=localhost,127.0.0.1,::1,host.docker.internal,qdrant,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16


# ------------------------------------------------------------------------------
# [实验性] 高级功能
# ------------------------------------------------------------------------------

ENABLE_GRAPH_MEMORY=false      # 图记忆 (实验性)
